{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8720a44-948d-4a11-a00b-1049b58f533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| gt|count|\n",
      "+---+-----+\n",
      "+---+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|10452|\n",
      "|       sit|12310|\n",
      "|     stand|11385|\n",
      "|      walk|13256|\n",
      "|      bike|10797|\n",
      "|stairsdown| 9365|\n",
      "|      null|10447|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|20905|\n",
      "|       sit|24620|\n",
      "|     stand|22770|\n",
      "|      walk|26512|\n",
      "|      bike|21594|\n",
      "|stairsdown|18729|\n",
      "|      null|20894|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|31357|\n",
      "|       sit|36929|\n",
      "|     stand|34154|\n",
      "|      walk|39768|\n",
      "|      bike|32390|\n",
      "|stairsdown|28094|\n",
      "|      null|31343|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|31357|\n",
      "|       sit|36929|\n",
      "|     stand|34154|\n",
      "|      walk|39768|\n",
      "|      bike|32390|\n",
      "|stairsdown|28094|\n",
      "|      null|31343|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|31357|\n",
      "|       sit|36929|\n",
      "|     stand|34154|\n",
      "|      walk|39768|\n",
      "|      bike|32390|\n",
      "|stairsdown|28094|\n",
      "|      null|31343|\n",
      "+----------+-----+\n",
      "\n",
      "+----------+-----+\n",
      "|        gt|count|\n",
      "+----------+-----+\n",
      "|  stairsup|31357|\n",
      "|       sit|36929|\n",
      "|     stand|34154|\n",
      "|      walk|39768|\n",
      "|      bike|32390|\n",
      "|stairsdown|28094|\n",
      "|      null|31343|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkAdvancedOps\")\\\n",
    "        .master(\"spark://spark-master:7077\").getOrCreate() \n",
    "# df = spark.read.format(\"csv\")\\\n",
    "#           .option(\"header\", \"true\")\\\n",
    "#           .option(\"inferSchema\", \"true\")\\\n",
    "#           .load(\"/home/jovyan/data/airline3\")\\\n",
    "#           .coalesce(5)\n",
    "\n",
    "\n",
    "df = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\"prefab-clover-330908.airline_2.correct_input_csv_batch\")   \n",
    "\n",
    "\n",
    "df1 = df\n",
    "df2 = df\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "sub_total_dest = df1.groupBy(\"Month\", \"Week_number\", \"Dest\").agg(count(\"NASDelay\").alias(\"sub_total_NASDelay\")).na.fill(0).collect()\n",
    "df_sub_total_dest = spark.createDataFrame(sub_total_dest)\n",
    "\n",
    "count_dest = df1.groupBy(\"Month\", \"Week_number\", \"Dest\").count().collect()\n",
    "df_count_dest = spark.createDataFrame(count_dest)\n",
    "\n",
    "mergedf_dest = df_sub_total_dest.join(df_count_dest, [\"Month\", \"Week_number\", \"Dest\"]).collect()\n",
    "df_mergedf_dest = spark.createDataFrame(mergedf_dest)\n",
    "df_mergedf_dest = df_mergedf_dest.withColumnRenamed(\"Dest\", \"City\").withColumnRenamed(\"count\", \"count1\").withColumnRenamed(\"sub_total_NASDelay\", \"s_t_NASDelay1\")\n",
    "\n",
    "sub_total_origin = df2.groupBy(\"Month\", \"Week_number\", \"Origin\").agg(count(\"NASDelay\").alias(\"sub_total_NASDelay\")).na.fill(0).collect()\n",
    "df_sub_total_origin = spark.createDataFrame(sub_total_origin)\n",
    "\n",
    "count_org = df2.groupBy(\"Month\", \"Week_number\", \"Origin\").count().collect()\n",
    "df_count_org = spark.createDataFrame(count_org)\n",
    "\n",
    "mergedf_org = df_sub_total_origin.join(df_count_org, [\"Month\", \"Week_number\", \"Origin\"]).collect()\n",
    "df_mergedf_org = spark.createDataFrame(mergedf_org)\n",
    "df_mergedf_org = df_mergedf_org.withColumnRenamed(\"Origin\", \"City\").withColumnRenamed(\"count\", \"count2\").withColumnRenamed(\"sub_total_NASDelay\", \"s_t_NASDelay2\")\n",
    "\n",
    "total_merge = df_mergedf_dest.join(df_mergedf_org, [\"Month\", \"Week_number\", \"City\"])\n",
    "\n",
    "from pyspark.sql.functions import lit, col\n",
    "df_total = total_merge.withColumn(\"total_nr_flights\", lit(col(\"count1\") + col(\"count2\")))\n",
    "drop_flights = df_total.where((col(\"total_nr_flights\") >= 10) ).select(\"*\") # use only the cities that got 10 or more flights so that only\n",
    "# airports with atleast some traffic are selected\n",
    "\n",
    "df_total_dr = drop_flights.drop(\"count1\", \"count2\")\n",
    "\n",
    "df_total_2 = df_total_dr.withColumn(\"total_NAS_delay\", lit(col(\"s_t_NASDelay1\") + col(\"s_t_NASDelay2\")))\n",
    "\n",
    "df_total_2_dr = df_total_2.drop(\"s_t_NASDelay1\", \"s_t_NASDelay2\")\n",
    "\n",
    "final_output = df_total_2_dr.withColumn(\"avg_NASDelay\", col(\"total_NAS_delay\")/ col(\"total_nr_flights\"))\n",
    "\n",
    "minutes_to_seconds = final_output.withColumn(\"avg_NASDelay_sec\", round(col(\"avg_NASDelay\") * 60))\n",
    "#minutes_to_seconds.show()\n",
    "\n",
    "final_output_2 = minutes_to_seconds.drop(\"total_nr_flights\", \"total_NAS_delay\", \"avg_NASDelay\")\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row, Window\n",
    "\n",
    "windowdesc = Window.partitionBy(col(\"Week_number\")).orderBy(col(\"avg_NASDelay_sec\").desc())\n",
    "\n",
    "airline_merged_windowed = final_output_2.withColumn(\"rank_desc\", dense_rank().over(windowdesc))\n",
    "#airline_merged_windowed.show()\n",
    "\n",
    "airline2 = airline_merged_windowed.where((col(\"rank_desc\") <= 10) ).select(\"*\")\n",
    "#airline2.show()\n",
    "\n",
    "airline2_table2 = airline2.select(\"Month\", \"Week_number\", \"City\", \"avg_NASDelay_sec\", \"rank_desc\").withColumnRenamed(\"rank_desc\", \"Worst_performing_cities\")\n",
    "\n",
    "worst_performing_output = airline2_table2\n",
    "\n",
    "\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "\n",
    "bucket = \"airplane_chris_ass2_batch\"    \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "\n",
    "# Saving the data to BigQuery\n",
    "worst_performing_output.write.format('bigquery') \\\n",
    "  .option('table', 'prefab-clover-330908.airline_2.worst_performing') \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d06d26-572b-473c-99a5-87d3bd4156d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
