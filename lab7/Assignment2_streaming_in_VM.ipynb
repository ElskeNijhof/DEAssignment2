{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8720a44-948d-4a11-a00b-1049b58f533a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "| key|               value|topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "|null|[C3 AF C2 BB C2 B...| word|        0|     0|2021-10-31 10:02:...|            0|\n",
      "|null|                [0A]| word|        0|     1|2021-10-31 10:02:...|            0|\n",
      "|null|[4D 61 6E 79 20 6...| word|        0|     2|2021-10-31 10:02:...|            0|\n",
      "|null|                [0A]| word|        0|     3|2021-10-31 10:02:...|            0|\n",
      "|null|[49 6E 20 61 20 6...| word|        0|     4|2021-10-31 10:02:...|            0|\n",
      "|null|                [0A]| word|        0|     5|2021-10-31 10:02:...|            0|\n",
      "|null|[54 68 65 20 62 6...| word|        0|     6|2021-10-31 10:02:...|            0|\n",
      "|null|                [0A]| word|        0|     7|2021-10-31 10:02:...|            0|\n",
      "|null|[41 73 20 72 65 7...| word|        0|     8|2021-10-31 10:02:...|            0|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import window, col, avg\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, DoubleType, ShortType, FloatType, ByteType, IntegerType\n",
    "from time import sleep\n",
    "\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"Lab7_1\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "# create the spark session, which is the entry point to Spark SQL engine.\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "dataSchema = StructType(\n",
    "        [StructField(\"ArrDelay\", FloatType(), True),\n",
    "         StructField(\"ArrTime\", FloatType(), True),\n",
    "         StructField(\"DepDelay\", FloatType(), True),\n",
    "         StructField(\"Dest\", StringType(), True),\n",
    "         StructField(\"Timestamp\", LongType(), True)            \n",
    "         ])\n",
    "\n",
    "#Read from a source \n",
    "sdf = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1) \\\n",
    "         .csv(\"/home/jovyan/data/airline/\")\n",
    "\n",
    "# create the event time column \n",
    "newsdf = sdf.selectExpr(\n",
    "    \"*\",\n",
    "    \"cast(Timestamp as timestamp) as event_time\") \n",
    "#    \"cast(cast(Timestamp as double) * 60 as timestamp) as event_time\")\n",
    "    \n",
    "newsdf.printSchema()\n",
    "\n",
    "\n",
    "# result = newsdf.groupBy(window(col(\"event_time\"), \"10 seconds\"),\"ArrTime\", \"Dest\").avg(\"ArrDelay\", \"DepDelay\") \\\n",
    "#         .writeStream \\\n",
    "#         .queryName(\"avg_arr_dep_delay_per_dest\") \\\n",
    "#         .format(\"memory\") \\\n",
    "#         .outputMode(\"complete\") \\\n",
    "#         .start()\n",
    "\n",
    "result = newsdf.groupBy(window(col(\"event_time\"), \"10 seconds\"),\"ArrTime\", \"Dest\").avg(\"ArrDelay\", \"DepDelay\")\n",
    "\n",
    "\n",
    "# Setup hadoop fs configuration for schema gs://\n",
    "conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "conf.set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "\n",
    "bucket = \"airplane_chris_ass2\"    \n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "# result.writeStream.format('bigquery') \\\n",
    "#   .option('table', 'prefab-clover-330908.airline_2.stream_chris_ass') \\\n",
    "#   .outputMode(\"complete\") \\\n",
    "\n",
    "\n",
    "def my_foreach_batch_function(df, batch_id):\n",
    "   # Saving the data to BigQuery as batch processing sink -see, use write(), save(), etc.\n",
    "    df.write.format('bigquery') \\\n",
    "      .option('table', 'prefab-clover-330908.airline_2.streaming_avg') \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save()\n",
    "\n",
    "airlineQuery = result.writeStream.outputMode(\"complete\") \\\n",
    "                    .trigger(processingTime = '2 seconds').foreachBatch(my_foreach_batch_function).start()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    airlineQuery.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    airlineQuery.stop()\n",
    "    # Stop the spark context\n",
    "    spark.stop()\n",
    "    print(\"Stoped the streaming query and the spark context\")\n",
    "# try:\n",
    "#     for x in range(10):\n",
    "#         spark.sql(\"SELECT * FROM avg_arr_dep_delay_per_dest\").show()\n",
    "#         sleep(10)\n",
    "# except KeyboardInterrupt:\n",
    "#         result.stop()\n",
    "#         # stop the Spark context\n",
    "#         print(\"Stopped the datastream\")\n",
    "#result.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d06d26-572b-473c-99a5-87d3bd4156d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the spark context\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
